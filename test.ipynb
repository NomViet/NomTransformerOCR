{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58939160-ea2f-49ad-94c9-4eef4224e4e8",
   "metadata": {},
   "source": [
    "# Test single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa012e2b-e9d7-42a1-8f04-542a852b936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193b69ad-070b-4a59-8ca5-157447709fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bao/JapanLLM/Qwen3-Vl/LLaMA-Factory/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from comer.datamodule import vocab\n",
    "from comer.lit_comer import LitCoMER\n",
    "\n",
    "ckpt = \"lightning_logs/version_48/checkpoints/epoch=259-step=61880-val_ExpRate=0.4218.ckpt\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = LitCoMER.load_from_checkpoint(ckpt, map_location=device)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = \"comer_data/data/val/img/val000001.bmp\"\n",
    "\n",
    "img = Image.open(image_path)\n",
    "img_tensor = ToTensor()(img).to(device)\n",
    "mask = torch.zeros_like(img_tensor, dtype=torch.bool)\n",
    "\n",
    "with torch.no_grad():\n",
    "    hyp = model.approximate_joint_search(img_tensor.unsqueeze(0), mask)[0]\n",
    "\n",
    "pred_indices = hyp.seq\n",
    "words = [\n",
    "    vocab.idx2word[i]\n",
    "    for i in pred_indices\n",
    "    if i not in (vocab.PAD_IDX, vocab.SOS_IDX, vocab.EOS_IDX)\n",
    "]\n",
    "\n",
    "print(\"\".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019902c4-afcc-4faf-b6fd-f43a5a9983c3",
   "metadata": {},
   "source": [
    "# Test Batch infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "824dbf64-f731-4b68-80c0-7ec38d44b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1: 及 諸 官 門 令 人 守 之 百 官 進 朝 不 得 入 守 度 衛 符\n",
      "Image 2: 人 心 中 異 之 帝 意 自 端 縣 不 擊 莫 氣 使 政\n",
      "Image 3: 有 天 下 者 順 也 及 李 之 代 黎 陳 之 代 李 以 有\n"
     ]
    }
   ],
   "source": [
    "# Load nhiều ảnh  \n",
    "device = \"cuda\"\n",
    "img_paths = ['comer_data/data/val/img/val121.bmp','comer_data/data/val/img/val111.bmp','comer_data/data/val/img/val116.bmp']  \n",
    "imgs = []  \n",
    "masks = []  \n",
    "  \n",
    "for img_path in img_paths:  \n",
    "    img = Image.open(img_path)  \n",
    "    img_tensor = ToTensor()(img)  \n",
    "    imgs.append(img_tensor)  \n",
    "  \n",
    "# Tìm kích thước lớn nhất  \n",
    "heights = [img.size(1) for img in imgs]  \n",
    "widths = [img.size(2) for img in imgs]  \n",
    "max_height = max(heights)  \n",
    "max_width = max(widths)  \n",
    "  \n",
    "# Tạo batch với padding  \n",
    "batch_size = len(imgs)  \n",
    "batch_imgs = torch.zeros(batch_size, 1, max_height, max_width)  \n",
    "batch_masks = torch.ones(batch_size, max_height, max_width, dtype=torch.bool)  \n",
    "  \n",
    "# Copy từng ảnh vào batch và cập nhật mask  \n",
    "for idx, img in enumerate(imgs):  \n",
    "    h, w = heights[idx], widths[idx]  \n",
    "    batch_imgs[idx, :, :h, :w] = img  \n",
    "    batch_masks[idx, :h, :w] = False  # False = vùng ảnh thực  \n",
    "  \n",
    "# Chuyển sang device và inference  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "batch_imgs = batch_imgs.to(device)  \n",
    "batch_masks = batch_masks.to(device)  \n",
    "  \n",
    "hyps = model.approximate_joint_search(batch_imgs, batch_masks)\n",
    "  \n",
    "# Chuyển đổi kết quả  \n",
    "for i, hyp in enumerate(hyps):  \n",
    "    pred_latex = vocab.indices2label(hyp.seq)  \n",
    "    print(f\"Image {i+1}: {pred_latex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8adf442-471b-46b6-8c1c-3b200f0bb739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (llamafactory)",
   "language": "python",
   "name": "llamafactory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
